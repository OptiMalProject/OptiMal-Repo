{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Markov Chains\n",
    "Markov Chain is a very important concept in the field of machine learning. It is defined as a random process that transitions from one state to another in the state space. That is, at each step of the Markov chain, the system can choose to change their state to a different one or remain the same according to a probability distribution. A change in state is called a transition, and the probabilities associated with different state changes are called transition probabilities. The probability distribution of the process depends only on the current state, not the events preceding it. This is known as the 'memoryless property' as well as the 'Markov property'. With this property, the modeling process of many problems is greatly simplified as it reduces complex dependencies to only one state.\n",
    "\n",
    "<img \n",
    "    style=\"display: block; \n",
    "           margin-left: auto;\n",
    "           margin-right: auto;\n",
    "           width: 50%;\"\n",
    "    src=\"../Examples/Markov_Chain.png\" \n",
    "    alt=\"markov_chain\">\n",
    "</img>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0806b23beb98121"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation\n",
    "\n",
    "We can start our implementation by viewing an agent's gameplay as a Markov chain. Import `numpy` library, and define the transition probability of weather."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a21ce58fbd21feef"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability: \n",
      "       left  right  fire\n",
      "left    0.7    0.1   0.2\n",
      "right   0.4    0.2   0.4\n",
      "fire    0.3    0.5   0.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# agent can take three actions: go to left, go to right and fire\n",
    "agent_states = [\"left\", \"right\", \"fire\"]\n",
    "transition_matrix = [\n",
    "    [0.7, 0.1, 0.2], \n",
    "    [0.4, 0.2, 0.4], \n",
    "    [0.3, 0.5, 0.2]\n",
    "]\n",
    "df = pd.DataFrame(transition_matrix, index=agent_states, columns=agent_states)\n",
    "print(f\"Transition probability: \\n{df}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T13:28:50.867310100Z",
     "start_time": "2024-07-26T13:28:50.864754Z"
    }
   },
   "id": "6a996e7fca91ce5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we simulate the state transition process of Markov chain."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a84d6a1832eb09da"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left': {'left': 0.7, 'right': 0.1, 'fire': 0.2}, 'right': {'left': 0.4, 'right': 0.2, 'fire': 0.4}, 'fire': {'left': 0.3, 'right': 0.5, 'fire': 0.2}}\n",
      "Original state: left\n",
      "Current state: left\n",
      "Current state: fire\n",
      "Current state: fire\n",
      "Current state: right\n",
      "Current state: left\n",
      "Current state: left\n",
      "Current state: left\n",
      "Current state: left\n",
      "Current state: fire\n",
      "Current state: right\n"
     ]
    }
   ],
   "source": [
    "# use dictionary to make the process more readable\n",
    "def convert_to_dict(states, matrix):\n",
    "    transition_dict = {}\n",
    "    for i, state in enumerate(states):\n",
    "        transition_dict[state] = {states[j]: matrix[i][j] for j in range(len(states))}\n",
    "    return transition_dict\n",
    "\n",
    "transition_prob = convert_to_dict(agent_states, transition_matrix)\n",
    "print(transition_prob)\n",
    "\n",
    "def state_trans(cur_state, agent_states, transition_prob):\n",
    "    state = np.random.choice(agent_states, p=[transition_prob[cur_state][next_state] for next_state in agent_states])\n",
    "    return state\n",
    "    \n",
    "cur_state = \"left\"\n",
    "print(f\"Original state: {cur_state}\")\n",
    "step = 10\n",
    "for i in range(step):\n",
    "    next_state = state_trans(cur_state, agent_states, transition_prob)\n",
    "    print(f\"Current state: {next_state}\")\n",
    "    cur_state = next_state"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-26T13:28:55.301758500Z",
     "start_time": "2024-07-26T13:28:55.294255Z"
    }
   },
   "id": "8ebf2e574907fff4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try a more complex example on a Super Mario Bros level. First, read our prepared SMB level from \"/emamples\" file. A Markov chain model is constructed based on these level data. The code generates a Markov chain representing the probability of state transitions by recording how often a particular tile appears in different states. In this notebook, we only used one level as training data, more level can be added to this process to obtain a better result. The original code can be found at: [SMB_Markov](https://github.com/PCGML-Book/Mariolike-Markov-Level-Generation/tree/main)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e538e107050ed7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------', 1: '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------', 2: '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------', 3: '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------', 4: '----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------', 5: '----------------------------------------------------------------------------------g-----------------------------------------------------------------------------------------------------------------------', 6: '----------------------!---------------------------------------------------------SSSSSSSS---SSS!--------------@-----------SSS----S!!S--------------------------------------------------------##------------', 7: '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------###------------', 8: '-------------------------------------------------------------------------------g----------------------------------------------------------------------------------------------------------####------------', 9: '----------------------------------------------------------------1------------------------------------------------------------------------------------------------------------------------#####------------', 10: '----------------!---S@S!S---------------------tt---------tt------------------S@S--------------C-----SU----!--!--!-----S----------SS------#--#----------##--#------------SS!S------------######------------', 11: '--------------------------------------tt------tt---------tt-----------------------------------------------------------------------------##--##--------###--##--------------------------#######------------', 12: '----------------------------tt--------tt------tt---------tt----------------------------------------------------------------------------###--###------####--###-----tt--------------tt-########--------F---', 13: '---M-----------------g------tt--------tt-g----tt-----g-g-tt------------------------------------g-g--------k-----------------gg-g-g----####--####----#####--####----tt---------gg---tt#########--------#---', 14: 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX--XXXXXXXXXXXXXXX---XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX--XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX', 15: 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX--XXXXXXXXXXXXXXX---XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX--XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'}\n",
      "{'   ': {'X': 1.0}, 'X  ': {'X': 0.9844559585492227, '-': 0.015544041450777202}, '-  ': {'-': 0.5714285714285714, 'X': 0.42857142857142855}, '  X': {'X': 0.5, '-': 0.5}, 'XXX': {'X': 1.0}, 'X-X': {'-': 1.0}, '---': {'-': 0.9917570498915401, '!': 0.0021691973969631237, 'S': 0.004338394793926247, 'C': 0.0004338394793926247, '1': 0.0004338394793926247, 'g': 0.0004338394793926247, '@': 0.0004338394793926247}, '-X-': {'X': 0.5, '-': 0.3333333333333333, '#': 0.16666666666666666}, '-XX': {'-': 0.8405797101449275, 'M': 0.007246376811594203, 'g': 0.07246376811594203, 't': 0.043478260869565216, 'k': 0.007246376811594203, '#': 0.028985507246376812}, 'MXX': {'-': 1.0}, 'gXX': {'-': 0.8333333333333334, 'g': 0.16666666666666666}, 'tXX': {'t': 0.5, '-': 0.4166666666666667, '#': 0.08333333333333333}, '--X': {'-': 1.0}, 'kXX': {'-': 1.0}, '#XX': {'#': 0.8076923076923077, '-': 0.19230769230769232}, '#-X': {'-': 1.0}, '  -': {'-': 1.0}, '-M-': {'-': 1.0}, '--M': {'-': 1.0}, '-g-': {'-': 1.0}, '--g': {'-': 1.0}, '-t-': {'t': 0.6470588235294118, '-': 0.35294117647058826}, 'ttt': {'t': 1.0}, 't-t': {'-': 1.0}, '-k-': {'-': 1.0}, '--k': {'-': 1.0}, '-gg': {'-': 1.0}, '-#-': {'-': 0.7083333333333334, '#': 0.25, 'F': 0.041666666666666664}, '-##': {'#': 0.8666666666666667, '-': 0.13333333333333333}, '###': {'#': 0.8775510204081632, '-': 0.12244897959183673}, '#-#': {'-': 1.0}, '--#': {'-': 1.0}, 't#t': {'-': 1.0}, 'F-#': {'-': 1.0}, '-tt': {'-': 1.0}, '--t': {'-': 1.0}, '-F-': {'-': 1.0}, '--F': {'-': 1.0}, '!--': {'-': 0.6, 'S': 0.3, '!': 0.1}, 'S--': {'@': 0.07142857142857142, '!': 0.14285714285714285, '-': 0.2857142857142857, 'U': 0.03571428571428571, 'S': 0.4642857142857143}, '@--': {'S': 0.6666666666666666, '-': 0.3333333333333333}, 'C--': {'-': 1.0}, 'U--': {'-': 1.0}, '-!-': {'-': 1.0}, '--!': {'-': 1.0}, '-S-': {'-': 1.0}, '-@S': {'-': 1.0}, '-S@': {'-': 1.0}, '-!S': {'-': 1.0}, '-S!': {'-': 1.0}, '--S': {'-': 1.0}, '1--': {'-': 1.0}, '-C-': {'-': 1.0}, '--C': {'-': 1.0}, '-US': {'-': 1.0}, '--U': {'-': 1.0}, '-SS': {'-': 0.9166666666666666, 'g': 0.08333333333333333}, '-1-': {'-': 1.0}, '--1': {'-': 1.0}, 'g--': {'-': 1.0}, 'gSS': {'-': 1.0}, '-@-': {'-': 1.0}, '--@': {'-': 1.0}, '-!!': {'-': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "level = {}\n",
    "\n",
    "with open('../examples/Mario.txt', 'r') as f:\n",
    "\t\ty = 0\n",
    "\t\tfor line in f:\n",
    "\t\t\tlevel[y] = line.strip()\n",
    "\t\t\ty+=1\n",
    "\n",
    "print(level)\n",
    "\n",
    "# Extract Markov chain Counts from 'level'\n",
    "markovCounts = {} # Dictionary of (x-1, y), (x-1, y+1), (x, y+1)\n",
    "\n",
    "maxY = len(level)-1\n",
    "for y in range(maxY, -1, -1):\n",
    "\tfor x in range(0, len(level[y])-1):\n",
    "\n",
    "\t\t# This grabs the tile values to the left (west), below (south), and left and below (southwest)\n",
    "\t\twest = \" \"\n",
    "\t\tsouthwest = \" \"\n",
    "\t\tsouth = \" \"\n",
    "\n",
    "\t\tif x>0: \n",
    "\t\t\twest = level[y][x-1]\n",
    "\t\tif y<maxY: \n",
    "\t\t\tsouth = level[y+1][x-1]\n",
    "\t\tif x>0 and y<maxY: \n",
    "\t\t\tsouthwest = level[y+1][x]\n",
    "\n",
    "\t\tstate = west+southwest+south\n",
    "\n",
    "\t\tif not state in markovCounts.keys():\n",
    "\t\t\tmarkovCounts[state] = {}\n",
    "\t\tif not level[y][x] in markovCounts[state].keys():\n",
    "\t\t\tmarkovCounts[state][level[y][x]] = 0\n",
    "\n",
    "\t\t# Increments the number of times we see the tile value at location (x,y) given the state (the tile values at (x-1, y), (x-1, y+1), (x, y+1))\n",
    "\t\tmarkovCounts[state][level[y][x]] +=1.0\n",
    "\n",
    "# Normalize markov counts in order to approximate probability values\n",
    "markovProbabilities = {} # The representation of our Markov chain, a dictionary of dictionaries \n",
    "for state in markovCounts.keys():\n",
    "\tmarkovProbabilities[state] = {}\n",
    "\n",
    "\tsumVal = 0\n",
    "\tfor action in markovCounts[state].keys():\n",
    "\t\tsumVal+=markovCounts[state][action]\n",
    "\tfor action in markovCounts[state].keys():\n",
    "\t\tmarkovProbabilities[state][action] =markovCounts[state][action]/sumVal # Approximation of probability values of seeing tile value 'action' given the current 'state'\n",
    "\n",
    "print(markovProbabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T04:27:30.805603200Z",
     "start_time": "2024-07-29T04:27:30.780598300Z"
    }
   },
   "id": "a4a3340b7e751731"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a new SMB level based on the previously trained Markov chain model, generate a new game level and write it to a txt file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e87d5fa359e2b489"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import random\n",
    "new_level = {}\n",
    "\n",
    "# Parameters determining the size of the new\n",
    "maxY = 15 # will end up generating a new level with one height larger than this\n",
    "maxX = 100\n",
    "\n",
    "# Starting in the bottom left corner, we begin the generation process, going bottom to top then left to right\n",
    "for y in range(maxY, -1, -1):\n",
    "\tnew_level[y] =\"\"\n",
    "\tfor x in range(0, maxX): # We generate one tile at a time for each iteration of this inner loop\n",
    "\n",
    "\t\t# Grab the current state, the three dependent values\n",
    "\t\twest = \" \"\n",
    "\t\tsouthwest = \" \"\n",
    "\t\tsouth = \" \"\n",
    "\n",
    "\t\tif x>0: \n",
    "\t\t\twest = new_level[y][x-1]\n",
    "\t\tif y<maxY: \n",
    "\t\t\tsouth = new_level[y+1][x-1]\n",
    "\t\tif x>0 and y<maxY: \n",
    "\t\t\tsouthwest = new_level[y+1][x]\n",
    "\n",
    "\t\tstate = west+southwest+south\n",
    "\n",
    "\t\t# Query the Markov chain to see what tile value we should place at this tile location\n",
    "\t\tif state in markovProbabilities.keys():\n",
    "\t\t\t\n",
    "\t\t\t# Greedy Sampling. \n",
    "\t\t\t# Uncomment this and comment the Weighted Sampling section below to see what greedy sampling looks like (and why we don't tend to use it)\n",
    "\t\t\t'''\n",
    "\t\t\tmaxValueTile = \"-\"\n",
    "\t\t\tmaxValue = 0.0\n",
    "\t\t\tfor action in markovProbabilities[state]:\n",
    "\t\t\t\tif markovProbabilities[state][action] >maxValue:\n",
    "\t\t\t\t\tmaxValue = markovProbabilities[state][action]\n",
    "\t\t\t\t\tmaxValueTile = action\n",
    "\t\t\tnew_level[y] +=maxValueTile #Add the tile value (tokenToUse) to the level\n",
    "\t\t\t'''\n",
    "\t\t\t# Weighted Sampling\n",
    "\t\t\trandValue = random.random()\n",
    "\t\t\tcurrProb = 0\n",
    "\t\t\ttokenToUse = \"-\"\n",
    "\t\t\tfor action in markovProbabilities[state]:\n",
    "\t\t\t\tcurrProb+=markovProbabilities[state][action]\n",
    "\t\t\t\tif currProb>randValue:\n",
    "\t\t\t\t\ttokenToUse = action\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tnew_level[y] += tokenToUse # Add the tile value (tokenToUse) to the level\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\t# If we can't find anything, just output an empty space\n",
    "\t\t\tnew_level[y] +=\"-\"\n",
    "\n",
    "f = open(\"../examples/New_Level.txt\",\"w\")\n",
    "for y in range(0, maxY+1):\n",
    "\tf.write(new_level[y]+\"\\n\")\n",
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T04:46:24.694755800Z",
     "start_time": "2024-07-29T04:46:24.674751500Z"
    }
   },
   "id": "aeda0e6624f2682"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Best practices\n",
    "- Not all problems can be represented as a markov chain, such as fluctuations in stoke market prices. Markov chain is a powerful method, but the applicable scenarios for your problems must be determined precisely.\n",
    "- The definition of state space should cover as many possible states as possible, but not so complex as to raise the computation cost. Also, they should be clear enough and independent of each other.\n",
    "- Ensure the sum of the transition probability matrix of each state to other states equals 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6540dbad54162370"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
