{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prompts for Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary libraries\n",
    "\n",
    "In this example, we will use the OpenAI API to interface with an LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "OpenAI API access requires an API key (from (here)[https://platform.openai.com]). Make sure to top up your balance before executing the queries to the LLM.\n",
    "\n",
    "Alternatively, some open-source LLMs now follow the same OpenAI API, which you can run locally using programs such as (LM Studio)[https://lmstudio.ai/]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'your-api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4'\n",
    "temperature = 0.5\n",
    "top_p = 0.\n",
    "rng_seed = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define a \"system\" prompt, which will determine the base behaviour of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant that always replies in rhymes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then send a single message to the LLM, and get a response back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.chat.completions import create\n",
    "\n",
    "user_message = 'What is the purpose of life?'\n",
    "\n",
    "messages = {\n",
    "\t{\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "}\n",
    "\n",
    "output = create(model=model_name,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                messages=messages,\n",
    "                seed=rng_seed\n",
    "                ).choices[0].message\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep a conversation history, simply append the assistant response to `messages` and loop until the user wants to quit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\n",
    "\t{\"role\": \"system\", \"content\": system_prompt}\n",
    "}\n",
    "\n",
    "while True:\n",
    "    user_message = input('You> ')\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    output = create(model=model_name,\n",
    "\t\t\t\t\ttemperature=temperature,\n",
    "\t\t\t\t\ttop_p=top_p,\n",
    "\t\t\t\t\tmessages=messages,\n",
    "\t\t\t\t\tseed=rng_seed\n",
    "\t\t\t\t\t).choices[0].message\n",
    "    print('AI> ', output.content)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": output.content})"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
