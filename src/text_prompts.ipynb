{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Prompts for Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use the OpenAI API to interface with an LLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "OpenAI API access requires an API key (from [here](https://platform.openai.com)). Make sure to top up your balance before executing the queries to the LLM.\n",
    "\n",
    "Alternatively, some open-source LLMs now follow the same OpenAI API, which you can run locally using programs such as [LM Studio](https://lmstudio.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'your-api-key-here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define LLM properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-4'\n",
    "temperature = 0.5\n",
    "top_p = 0.\n",
    "rng_seed = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can define a \"system\" prompt, which will determine the base behaviour of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant that always replies in rhymes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then send a single message to the LLM, and get a response back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.chat.completions import create\n",
    "\n",
    "user_message = 'What is the purpose of life?'\n",
    "\n",
    "messages = {\n",
    "\t{\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "}\n",
    "\n",
    "output = create(model=model_name,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                messages=messages,\n",
    "                seed=rng_seed\n",
    "                ).choices[0].message\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep a conversation history, simply append the assistant response to `messages` and loop until the user wants to quit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\n",
    "\t{\"role\": \"system\", \"content\": system_prompt}\n",
    "}\n",
    "\n",
    "while True:\n",
    "    user_message = input('You> ')\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    output = create(model=model_name,\n",
    "\t\t\t\t\ttemperature=temperature,\n",
    "\t\t\t\t\ttop_p=top_p,\n",
    "\t\t\t\t\tmessages=messages,\n",
    "\t\t\t\t\tseed=rng_seed\n",
    "\t\t\t\t\t).choices[0].message\n",
    "    print('AI> ', output.content)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": output.content})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM responses can be used directly as genotypes for different kinds of content, or define changes to be applied to existing content.\n",
    "\n",
    "In this simple example, we will use in-context learning to let the LLM update a string according to user-defined rules until a certain condition is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_with_llm(x: str,\n",
    "                    rules) -> str:\n",
    "    rules_str = '\\n'.join([f'{i}. Replace \\'{a}\\' with \\'{b}\\'' for i, (a, b) in enumerate(rules)])\n",
    "    messages = [\n",
    "\t\t{'role': 'system', 'content': \"Update the string provided by the users according to the following rules:\\n\"\n",
    "\t\t\t\t\t\t\t\t\t  f\"{rules_str}\\n\"\n",
    "                    \t\t\t\t  \"Only return the updated string.\"},\n",
    "\t\t{'role': 'user', 'content': x}\n",
    "\t]\n",
    "    output = create(model=model_name,\n",
    "\t\t\t\t\ttemperature=temperature,\n",
    "\t\t\t\t\ttop_p=top_p,\n",
    "\t\t\t\t\tmessages=messages,\n",
    "\t\t\t\t\tseed=rng_seed\n",
    "\t\t\t\t\t).choices[0].message\n",
    "    return output.content\n",
    "\n",
    "def is_terminated(x: str) -> bool:\n",
    "    return 'c' in x\n",
    "\n",
    "def process_and_log(eval_str, rules):\n",
    "    iters = 0\n",
    "    while not is_terminated(eval_str):\n",
    "        eval_str = update_with_llm(eval_str)\n",
    "        iters += 1\n",
    "    print(f'Obtained \\'{eval_str}\\' after {iters} iterations.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_string = 'aaaaabababaaababa'\n",
    "rules = [\n",
    "\t('aba', 'bbb')\n",
    "\t('bbb', 'c')\n",
    "]\n",
    "\n",
    "process_and_log(starting_string, rules)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
